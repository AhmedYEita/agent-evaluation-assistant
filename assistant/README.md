# Agent Evaluation Setup Assistant

An **ADK-based conversational agent** that helps users set up agent evaluation infrastructure.

## Quick Start

### Step 1: Clone the SDK Repository Inside Your Agent Project

Clone this repo **outside** your agent project directory:

```bash
# Clone in a separate location (e.g., ~/repos, ~/projects)
cd ~/repos
git clone https://github.com/AhmedYEita/agent-evaluation-assistant
cd agent-evaluation-assistant
pip install -e ./sdk
```

**Important:** Keep the SDK repo **separate** from your agent project:
```
~/repos/
â”œâ”€â”€ agent-evaluation-assistant/     # â† SDK repo (clone here)
â””â”€â”€ my-agent-project/           # â† Your agent (existing project)
```

### Step 2: Run the Assistant

**Direct:**
```bash
cd assistant/agent
pip install -r requirements.txt
export GOOGLE_CLOUD_PROJECT="YOUR GCP PROJECT"
export GOOGLE_CLOUD_REGION="us-central1"
python assistant_agent.py
```

**Via CLI:**
```bash
agent-eval-assistant setup
# (Directs you to run the assistant agent)
```

### Step 3: Follow the Interactive Setup

The assistant will ask for your agent project path (e.g., `~/repos/my-agent-project`) and copy the necessary files there.

---

## What the Assistant Does

The assistant **conversationally guides** you through:

1. âœ… Getting your agent project path
2. âœ… Verifying agent compatibility (checks for `generate_content()` method)
3. âœ… Configuring observability services (logging, tracing, metrics)
4. âœ… Setting up dataset collection
5. âœ… Generating customized `eval_config.yaml` in your project
6. âœ… Copying terraform module to your project
7. âœ… Showing integration code with your specific values

---

## Architecture

### **ADK-Based Agent**

```
User â†â†’ Assistant Agent (Google ADK) â†â†’ Tools
         â†‘
         System Instructions
```

**Built with:**
- Google ADK (Agent Development Kit)
- Gemini 2.0 Flash model
- System instructions for personality and workflow
- Python tools for file operations

**Not a script!** The assistant adapts to your responses and asks intelligent follow-up questions.

See [ADK_ARCHITECTURE.md](./ADK_ARCHITECTURE.md) for detailed explanation.

---

## Files Structure

```
assistant/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ assistant_agent.py        # ADK agent (140 lines)
â”‚   â”œâ”€â”€ requirements.txt           # google-adk, pyyaml, bigquery
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ file_operations.py    # Copy files, check repo/agent
â”‚       â”œâ”€â”€ config_validator.py   # Validate YAML
â”‚       â””â”€â”€ infra_checker.py      # Check BigQuery/GCP
â”‚
â”œâ”€â”€ ADK_ARCHITECTURE.md           # Detailed architecture explanation
â””â”€â”€ README.md                     # This file
```

---

## Example Conversation

```
ğŸ¤– Assistant: Hi! ğŸ‘‹ I'm here to help you set up agent evaluation infrastructure.
              This will take about 5-7 minutes. Ready to get started?

You: Yes

ğŸ¤– Assistant: Great! What's the path to your agent project where you
              want to set up evaluation? (e.g., ~/my-agent-project)

You: ~/my-chatbot

ğŸ¤– Assistant: Got it! Can you provide the path to your agent file that has the
              generate_content() method? (e.g., ~/my-chatbot/agent.py)

You: ~/my-chatbot/chatbot_agent.py

ğŸ¤– Assistant: Let me check if your agent is compatible...
              âœ“ Agent looks good! Found generate_content() method.
              
              Now, let's configure observability services. I'll explain each one:
              
              ğŸ“Š **Logging**: Captures all agent interactions and errors
              ğŸ“ˆ **Tracing**: Tracks request flow and latency
              ğŸ“‰ **Metrics**: Monitors performance over time
              
              Which services would you like to enable? (all/logging/tracing/metrics)

You: all

ğŸ¤– Assistant: Excellent choice! âœ“ Enabling all observability services.
              
              ğŸ’¡ One more thing: **Dataset Collection**
              
              This automatically saves agent interactions to BigQuery for evaluation.
              I recommend keeping it OFF by default (you can enable it later for
              specific test runs).
              
              Enable dataset collection? (yes/no)

You: no

ğŸ¤– Assistant: âœ“ Smart! Dataset collection will be disabled by default.
              
              What's your GCP Project ID?

You: my-gcp-project

ğŸ¤– Assistant: And what would you like to name your agent? (e.g., chatbot-agent)

You: my-chatbot

ğŸ¤– Assistant: Perfect! Let me generate your configuration...
              
              âœ“ Created eval_config.yaml in ~/my-chatbot/
              âœ“ Copied terraform module to ~/my-chatbot/terraform/
              
              ğŸ“ **Next Steps:**
              
              1. **Integrate the SDK into your agent:**
              
              ```python
              from agent_evaluation_sdk import enable_evaluation
              
              # Wrap your agent
              wrapped_agent = enable_evaluation(
                  your_agent,
                  "eval_config.yaml"
              )
              
              # Use it
              response = wrapped_agent.generate_content("Hello!")
              ```
              
              2. **Deploy infrastructure:**
              
              ```bash
              cd ~/my-chatbot/terraform
              terraform init
              terraform apply
              ```
              
              3. **Test it:**
              
              ```bash
              python your_agent.py
              ```
              
              Check Cloud Logging and BigQuery for your data! ğŸ‰
              
              Need help with anything else?

You: No, thanks!

ğŸ¤– Assistant: You're all set! ğŸš€ Happy building!
```

---

## CLI Validation

Quick check if your setup is correct:

```bash
agent-eval-assistant validate --project ~/my-chatbot
```

Output:

```
ğŸ” Setup Validation
============================================================
Checking: /Users/you/my-chatbot

âœ“ eval_config.yaml found
âœ“ Valid YAML syntax
âœ“ Terraform module found

âœ“ Everything looks good! ğŸ‰
```

---

## Key Features

### ğŸ§  Conversational Intelligence

- Adapts to user responses
- Explains WHY things matter
- Provides trade-offs and recommendations
- Friendly, not robotic

### ğŸ”§ Automated Setup

- Validates agent compatibility
- Generates customized configs
- Copies necessary files
- Shows integration code with real values

### âœ… Smart Defaults

- Observability services: All enabled
- Dataset collection: Disabled by default
- Region: us-central1
- Model: gemini-2.0-flash

### ğŸ›¡ï¸ Error Handling

- Validates agent file structure
- Verifies YAML syntax
- Provides helpful error messages

---

## Tools Used by the Agent

The ADK agent uses these Python functions as tools:

| Tool | Purpose |
|------|---------|
| `check_agent_compatibility_tool` | Check for `generate_content()` |
| `validate_config_tool` | Validate YAML syntax |
| `check_infrastructure_tool` | Check GCP resources exist |
| `copy_config_template_tool` | Generate `eval_config.yaml` |
| `copy_terraform_module_tool` | Copy terraform files |

See [agent/tools/](./agent/tools/) for implementation details.

---

## Customization

You can modify the assistant's behavior by editing:

- **System Instruction** in `assistant_agent.py` - Change personality, workflow
- **Tools** in `agent/tools/` - Add new capabilities
- **Model** in `assistant_agent.py` - Use different Gemini version

---

## FAQ

### Q: Why run locally instead of deploying?

**A:** The assistant needs to access your local files to check your agent, copy configs, and set up terraform. Running locally ensures full functionality.

### Q: Can I use this for multiple projects?

**A:** Yes! Run the assistant once for each project. It will generate separate configs and terraform for each.

### Q: What if I don't use terraform?

**A:** The assistant still generates the terraform module for reference. You can deploy GCP resources manually using the module as a guide.

### Q: Can I skip certain steps?

**A:** Yes! The assistant is conversational. Just let it know what you want to skip.

---

## Next Steps

1. **Run the assistant** to set up your first agent
2. **Deploy infrastructure** with terraform
3. **Test your agent** and check Cloud Console
4. **Run evaluation** with `run_evaluation.py`

See the main [README.md](../README.md) for full documentation.
