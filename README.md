# Agent Evaluation Agent

**Production-ready evaluation infrastructure for AI agents with automated monitoring, testing, and quality evaluation.**

## Overview

A lightweight Python SDK and Terraform infrastructure that enables comprehensive evaluation for Google ADK agents with minimal code changes. Get structured logging, performance tracing, metrics dashboards, dataset collection, and quality testing by adding just one line of code to your agent.

### Key Features

- **One-Line Integration**: Enable full evaluation with a single function call
- **Zero-Latency Observability**: All Cloud API calls run in background threads
- **Automated Data Collection**: Logs, traces, metrics, and datasets captured automatically
- **Production-Ready**: Built on GCP services (Cloud Logging, Trace, Monitoring, BigQuery)
- **Quality Evaluation**: Vertex AI Gen AI Evaluation Service for automated and model-based metrics
- **Infrastructure as Code**: Reproducible Terraform deployment
- **Flexible Configuration**: Enable/disable services and tune performance per environment

## ğŸš€ Quick Start

### 1. Deploy Infrastructure (2 minutes)
```bash
cd terraform
terraform init
terraform apply -var="project_id=GCP_PROJECT_ID"
```

### 2. Configure Your Setup

**Agent Config** (`agent_config.yaml`):
```yaml
project_id: "GCP_PROJECT_ID"
agent_name: "my-agent"
model: "gemini-2.5-flash"
```

**SDK Config** (`eval_config.yaml`):
```yaml
logging:
  enabled: true
tracing:
  enabled: true
dataset:
  auto_collect: false
```

### 3. Enable Evaluation (1 line!)
```python
from agent_evaluation_sdk import enable_evaluation

# Your agent setup
agent = YourAgent(...)

# Enable evaluation
wrapper = enable_evaluation(agent, "GCP_PROJECT_ID", "my-agent", "eval_config.yaml")
```

## âœ¨ What You Get

### Real-time Monitoring (Automatic)
After `enable_evaluation()`, every agent interaction automatically gets:

âœ… **Structured Logging** - Every interaction logged to Cloud Logging  
âœ… **Performance Tracing** - Nested spans show LLM call, processing, and logging time  
âœ… **Real-time Metrics** - Pre-built dashboard in Cloud Monitoring  
âœ… **Error Tracking** - Automatic error capture in traces and logs  
âœ… **Dataset Collection** - Optional auto-capture to BigQuery

### Quality Testing (Manual)
Run `python run_evaluation.py` to test your agent:

ğŸ§ª **Regression Testing** - Test against historical dataset  
ğŸ“Š **Quality Metrics** - BLEU, ROUGE, coherence, fluency, safety  
ğŸ“ˆ **Performance Tracking** - Compare test runs over time  

## ğŸ”§ Technical Stack

- **Agent Framework**: Google ADK (Agent Development Kit)
- **Deployment Target**: Agent Engine (ready for deployment)
- **Infrastructure**: Terraform for GCP services
- **Monitoring**: Cloud Logging, Trace, Monitoring
- **Dataset Storage**: BigQuery for datasets
- **Evaluation**: Vertex AI Gen AI Evaluation Service
- **CI/CD**: GitHub Actions for validation

## ğŸ§ª Agent Testing & Evaluation

```bash
# 1. Enable dataset collection in eval_config.yaml
# 2. Run agent - interactions auto-collect to BigQuery
python custom_agent.py --test

# 3. Review in BigQuery - update 'reference' field, set 'reviewed = TRUE'
# 4. Run evaluation test
python run_evaluation.py
```

**Workflow:**
1. **Collect** - Agent responses stored in test dataset table
2. **Review** - Update ground truth in BigQuery
3. **Evaluate** - Run agent on test cases, compare responses vs ground truth

**Available Metrics:**
- **Automated**: BLEU, ROUGE
- **Model-based**: Coherence, Fluency, Safety, Groundedness, Fulfillment, Instruction Following, Verbosity

See [SETUP.md](./SETUP.md#agent-testing--evaluation) for details.


## ğŸ“ Repository Structure

```
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ SETUP.md                     # Complete setup and deployment guide
â”œâ”€â”€ CONTRIBUTING.md              # Contributing guidelines
â”œâ”€â”€ sdk/                         # Python SDK
â”‚   â”œâ”€â”€ agent_evaluation_sdk/    # Core SDK code
â”‚   â””â”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ terraform/                   # Infrastructure as Code
â”‚   â””â”€â”€ modules/                 # GCP services modules
â”œâ”€â”€ example_agents/              # Working examples
â”‚   â”œâ”€â”€ custom_agent.py          # Custom agent example
â”‚   â”œâ”€â”€ adk_agent.py             # ADK agent example
â”‚   â”œâ”€â”€ agent_config.yaml        # Agent-specific config
â”‚   â””â”€â”€ eval_config.yaml         # SDK config
â””â”€â”€ .github/workflows/           # CI/CD pipelines
```

## ğŸ“š Documentation

- **[SETUP.md](./SETUP.md)** - Complete setup and deployment guide
- **[CONTRIBUTING.md](./CONTRIBUTING.md)** - Development and contribution guidelines
- **[example_agents/](./example_agents/)** - Working code samples

## ğŸ“„ License

MIT License
