# Agent Evaluation Assistant

**Production-ready evaluation infrastructure for AI agents with one-line integration.**

## Overview

A Python SDK and Terraform infrastructure for comprehensive agent evaluation with minimal code changes. Get structured logging, performance tracing, metrics dashboards, dataset collection, and quality testing by adding a single line of code.

### Key Features

- **One-Line Integration**: `enable_evaluation(agent, project_id, agent_name, config)`
- **Setup Assistant**: Interactive ADK agent guides you through setup
- **Zero-Latency**: All Cloud API calls run in background threads
- **Automated Observability**: Logs, traces, metrics, and datasets captured automatically
- **Production-Ready**: Built on GCP services (Cloud Logging, Trace, Monitoring, BigQuery)
- **Quality Evaluation**: Vertex AI Gen AI Evaluation Service for automated and model-based metrics
- **Infrastructure as Code**: Reproducible Terraform deployment
- **Flexible Configuration**: Enable/disable services and tune performance
## ğŸš€ Quick Start

### 1. Clone & Install SDK (Separate from Your Agent)

Clone this repo **outside** your agent project directory:

```bash
cd ~/repos  # or wherever you keep repositories
git clone https://github.com/AhmedYEita/agent-evaluation-assistant
cd agent-evaluation-assistant
pip install -e ./sdk
```

**Important:** Keep the SDK repo **separate** from your agent project:
```
~/repos/
â”œâ”€â”€ agent-evaluation-assistant/     # â† SDK repo (clone here)
â””â”€â”€ my-agent-project/           # â† Your agent (existing project)
```

### 2. Run Setup Assistant (Recommended)

```bash
cd assistant/agent
pip install -r requirements.txt

# Set your GCP project (required for the assistant)
export GOOGLE_CLOUD_PROJECT="your-gcp-project-id"
export GOOGLE_CLOUD_REGION="us-central1"

python assistant_agent.py
```

The assistant will guide you through:
- âœ… Getting your agent project path
- âœ… Verifying agent compatibility
- âœ… Generating configuration files **in your project**
- âœ… Setting up Terraform infrastructure **in your project**
- âœ… Showing integration code

### 3. Enable Evaluation

```python
from agent_evaluation_sdk import enable_evaluation

agent = YourAgent(...)
wrapper = enable_evaluation(agent, "your-gcp-project-id", "agent-name", "eval_config.yaml")
```

That's it! Your agent now has full observability.

## What You Get

### Automatic Monitoring
- âœ… **Cloud Logging** - Every interaction logged with interaction_id, input, output, duration
- âœ… **Cloud Trace** - Nested spans show LLM calls, processing time, tool usage
- âœ… **Cloud Monitoring** - Pre-built dashboard with latency, errors, token usage
- âœ… **Dataset Collection** - Optional auto-capture to BigQuery for testing

### Quality Testing
- ğŸ§ª **Regression Testing** - Test against historical dataset
- ğŸ“Š **Automated Metrics** - BLEU, ROUGE scores
- ğŸ¯ **Model-Based Criteria** - Coherence, fluency, safety, groundedness
- ğŸ“ˆ **Performance Tracking** - Compare test runs over time

## Repository Structure

```
â”œâ”€â”€ sdk/                    # Python SDK (pip install -e ./sdk)
â”œâ”€â”€ assistant/              # Interactive setup assistant
â”œâ”€â”€ terraform/              # GCP infrastructure (BigQuery, Logging, Monitoring)
â”œâ”€â”€ example_agents/         # Working examples (custom + ADK agents)
â”œâ”€â”€ README.md              # This file - Overview & quick start
â”œâ”€â”€ SETUP.md               # Detailed setup & deployment guide
â””â”€â”€ CONTRIBUTING.md        # Development guidelines
```

## Documentation

Each file has a specific focus:

- **[SETUP.md](./SETUP.md)** - Complete setup guide (GCP, Terraform, configuration, troubleshooting)
- **[ROADMAP.md](./ROADMAP.md)** - Future enhancements (ADK plugin, A2A, PyPI distribution)
- **[assistant/README.md](./assistant/README.md)** - Setup assistant usage and architecture
- **[example_agents/README.md](./example_agents/README.md)** - Running example agents
- **[sdk/README.md](./sdk/README.md)** - SDK API reference
- **[CONTRIBUTING.md](./CONTRIBUTING.md)** - Development workflow

## Architecture Decisions

**Local Assistant:** Runs locally to automate file operations, validate code, and configure infrastructure (requires filesystem access).

**Wrapper Approach:** The SDK provides an evaluation wrapper that intercepts agent calls to capture observability data while running in background threads for zero-latency performance. Works universally with ADK agents, custom agents, and can extend to other frameworks.

**Compatibility Detection:** Discovers and scans all Python files in the agent directory (up to 4 levels deep) to detect ADK or Custom agent patterns, regardless of how code is organized across files.

**Manual Setup:** Prefer not to use the assistant? See [SETUP.md](./SETUP.md#manual-setup-alternative) for step-by-step manual configuration.

## Evaluation Workflow

```bash
# 1. Enable dataset collection
# Set auto_collect: true in eval_config.yaml

# 2. Run agent to collect data
python your_agent.py --test

# 3. Review & update reference answers in BigQuery
# Set reviewed=TRUE after verification

# 4. Disable collection
# Set auto_collect: false in eval_config.yaml

# 5. Run evaluation
python run_evaluation.py
```

## Technical Stack

- **Framework**: Google ADK (Agent Development Kit)
- **Infrastructure**: Terraform + GCP (Logging, Trace, Monitoring, BigQuery, Vertex AI)
- **Language**: Python 3.12+
- **CI/CD**: GitHub Actions
