# Agent Evaluation Assistant

**Production-ready evaluation infrastructure for AI agents with automated monitoring, testing, and quality evaluation.**

## Overview

A lightweight Python SDK and Terraform infrastructure that enables comprehensive evaluation for Google ADK agents with minimal code changes. Get structured logging, performance tracing, metrics dashboards, dataset collection, and quality testing by adding just one line of code to your agent.

### Key Features

- **One-Line Integration**: Enable full evaluation with a single function call
- **Setup Assistant**: Interactive CLI tool to guide you through setup
- **Zero-Latency Observability**: All Cloud API calls run in background threads
- **Automated Data Collection**: Logs, traces, metrics, and datasets captured automatically
- **Production-Ready**: Built on GCP services (Cloud Logging, Trace, Monitoring, BigQuery)
- **Quality Evaluation**: Vertex AI Gen AI Evaluation Service for automated and model-based metrics
- **Infrastructure as Code**: Reproducible Terraform deployment
- **Flexible Configuration**: Enable/disable services and tune performance per environment

## ğŸš€ Quick Start

### 1. Clone & Install SDK (Separate from Your Agent)

Clone this repo **outside** your agent project:

```bash
# Clone in a separate location (e.g., ~/repos, ~/projects)
cd ~/repos
git clone https://github.com/yourorg/agent-evaluation-assistant
cd agent-evaluation-assistant
pip install -e ./sdk
```

**Directory structure:**
```
~/repos/
â”œâ”€â”€ agent-evaluation-assistant/     # â† SDK repo (clone here)
â””â”€â”€ my-agent-project/           # â† Your agent (existing project)
```

### 2. Run Setup Assistant

```bash
cd agent-evaluation-assistant/assistant/agent
python assistant_agent.py
```

The assistant will guide you through:
- âœ… Getting your agent project path
- âœ… Verifying agent compatibility
- âœ… Generating configuration files **in your project**
- âœ… Setting up Terraform infrastructure **in your project**
- âœ… Showing integration code

**Or manually configure** by creating these files in your project:

**Agent Config** (`agent_config.yaml`):
```yaml
project_id: "GCP_PROJECT_ID"
agent_name: "my-agent"
  model: "gemini-2.5-flash"
```

**SDK Config** (`eval_config.yaml`):
```yaml
logging:
  enabled: true
tracing:
  enabled: true
dataset:
  auto_collect: false  # Set to true to collect data, then back to false
```

### 3. Enable Evaluation (1 line!)
```python
from agent_evaluation_sdk import enable_evaluation

agent = YourAgent(...)
wrapper = enable_evaluation(agent, "PROJECT_ID", "agent-name", "eval_config.yaml")
```

## âœ¨ What You Get

### Real-time Monitoring (Automatic)
After `enable_evaluation()`, every agent interaction automatically gets:

âœ… **Structured Logging** - Every interaction logged to Cloud Logging  
âœ… **Performance Tracing** - Nested spans show LLM call, processing, and logging time  
âœ… **Real-time Metrics** - Pre-built dashboard in Cloud Monitoring  
âœ… **Error Tracking** - Automatic error capture in traces and logs  
âœ… **Dataset Collection** - Optional auto-capture to BigQuery

### Quality Testing (Manual)
Run `python run_evaluation.py` to test your agent:

ğŸ§ª **Regression Testing** - Test against historical dataset  
ğŸ“Š **Quality Metrics** - BLEU, ROUGE, coherence, fluency, safety  
ğŸ“ˆ **Performance Tracking** - Compare test runs over time  

## ğŸ”§ Technical Stack

- **Agent Framework**: Google ADK (Agent Development Kit)
- **Deployment Target**: Agent Engine (ready for deployment)
- **Infrastructure**: Terraform for GCP services
- **Monitoring**: Cloud Logging, Trace, Monitoring
- **Dataset Storage**: BigQuery for datasets
- **Evaluation**: Vertex AI Gen AI Evaluation Service
- **CI/CD**: GitHub Actions for validation

## ğŸ§ª Agent Testing & Evaluation

```bash
# 1. Enable dataset collection in eval_config.yaml
# 2. Run agent - interactions auto-collect to BigQuery
python custom_agent.py --test

# 3. Review in BigQuery - update 'reference' field, set 'reviewed = TRUE'
# 4. Run evaluation test
python run_evaluation.py
```

**Workflow:**
1. **Collect** - Set `auto_collect: true`, run agent with `--test`, then set back to `false`
2. **Review** - Update ground truth in BigQuery (`{agent_name}_eval_dataset` table)
3. **Evaluate** - Run evaluation script (appends to `{agent_name}_eval_run` and `{agent_name}_eval_metrics` tables)

**Available Metrics:**
- **Automated**: BLEU, ROUGE
- **Model-based**: Coherence, Fluency, Safety, Groundedness, Fulfillment, Instruction Following, Verbosity

See [SETUP.md](./SETUP.md#agent-testing--evaluation) for details.


## ğŸ“ Repository Structure

```
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ SETUP.md                     # Complete setup and deployment guide
â”œâ”€â”€ CONTRIBUTING.md              # Contributing guidelines
â”œâ”€â”€ sdk/                         # Python SDK
â”‚   â”œâ”€â”€ agent_evaluation_sdk/    # Core SDK code
â”‚   â””â”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ assistant/                   # Setup Assistant (NEW!)
â”‚   â””â”€â”€ agent/                   # ADK assistant agent (run locally)
â”œâ”€â”€ terraform/                   # Infrastructure as Code
â”‚   â””â”€â”€ modules/                 # GCP services modules
â”œâ”€â”€ example_agents/              # Working examples
â”‚   â”œâ”€â”€ custom_agent.py          # Custom agent example
â”‚   â”œâ”€â”€ adk_agent.py             # ADK agent example
â”‚   â”œâ”€â”€ agent_config.yaml        # Agent-specific config
â”‚   â””â”€â”€ eval_config.yaml         # SDK config
â””â”€â”€ .github/workflows/           # CI/CD pipelines
```

## ğŸ“š Documentation

- **[SETUP.md](./SETUP.md)** - Complete setup and deployment guide
- **[CONTRIBUTING.md](./CONTRIBUTING.md)** - Development and contribution guidelines
- **[assistant/](./assistant/)** - Setup Assistant documentation
- **[example_agents/](./example_agents/)** - Working code samples

## ğŸ› ï¸ CLI Commands

```bash
# Interactive setup
agent-eval-assistant setup

# Validate existing setup
agent-eval-assistant validate --project /path/to/project
```

## ğŸ“„ License

MIT License
