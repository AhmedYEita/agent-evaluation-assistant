# Evaluation Configuration

# GCP Project Settings
project_id: "dt-ahmedyasser-sandbox-dev"  # Your actual GCP project ID
agent_name: "my_agent"         # Used for BigQuery table naming

# Agent Configuration
agent:
  model: "gemini-2.5-flash"

# Logging Configuration
logging:
  enabled: true
  level: "INFO"
  include_trajectories: true

# Tracing Configuration
tracing:
  enabled: true

# Metrics Configuration
metrics:
  enabled: true

# Dataset Collection Configuration
dataset:
  auto_collect: true  # When enabled, collects all interactions
  storage_location: null  # BigQuery table for storing collected interactions (null = auto-created table)
  buffer_size: 10  # Number of interactions to buffer before writing to BigQuery

# Gen AI Evaluation Configuration
genai_eval:
  metrics: ["bleu", "rouge"]  # Automated metrics
  model_name: "gemini-2.5-flash"
  criteria: ["coherence", "fluency", "safety", "groundedness", "fulfillment"]  # Pointwise evaluation criteria
  thresholds:   # Pass Rate Thresholds (0-1 scale) for a test case to be considered "passing"
    bleu: 0.5
    rouge: 0.5
    coherence: 0.7
    fluency: 0.7
    safety: 0.9
    groundedness: 0.7
    fulfillment: 0.6

# Regression Testing Configuration
regression:
  test_limit: null  # Max number of test cases (null = no limit)
  only_reviewed: true  # Only use reviewed test cases
  dataset_table: null  # Read from a custom BigQuery table for test cases (null = use default: {project_id}.agent_evaluation.{agent_name}_eval_dataset)

